{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nNxgk6o-NZZF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url=\"https://books.toscrape.com/catalogue/\"\n",
        "webpage = requests.get(url)\n",
        "webpage.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "vbzMZ_lERQhw",
        "outputId": "34f0941d-ecd2-42f3-c282-c0730bb612d9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<html>\\r\\n<head><title>403 Forbidden</title></head>\\r\\n<body>\\r\\n<center><h1>403 Forbidden</h1></center>\\r\\n<hr><center>nginx/1.21.6</center>\\r\\n</body>\\r\\n</html>\\r\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_books(url):\n",
        "    books = []\n",
        "    while url:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code != 200:\n",
        "            print(\"Failed to retrieve page\")\n",
        "            break\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        book_list = soup.find_all('article', class_='product_pod')\n",
        "        for book in book_list:\n",
        "            title = book.h3.a.attrs['title']\n",
        "            price = book.find('p', class_='price_color').text[1:]\n",
        "            availability = book.find('p', class_='instock availability').text.strip()\n",
        "            rating = book.p.attrs['class'][1]\n",
        "            books.append([title, price, availability, rating])\n",
        "        next_page = soup.find('li', class_='next')\n",
        "        if next_page:\n",
        "            url = \"https://books.toscrape.com/catalogue/\" + next_page.a.attrs['href']\n",
        "        else:\n",
        "            url = None\n",
        "    return books"
      ],
      "metadata": {
        "id": "9MFgBqDaNuTc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_url = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
        "book_data = scrape_books(start_url)"
      ],
      "metadata": {
        "id": "mZoUwzMHOg0s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(book_data, columns=['Title', 'Price (GBP)', 'Availability', 'Rating'])"
      ],
      "metadata": {
        "id": "Azp20kFAOtoF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"books_dataset.csv\", index=False)\n",
        "\n",
        "print(\"Scraping complete. Dataset saved as 'books_dataset.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4K-eGP4OxeR",
        "outputId": "8df61190-9f47-46e3-ebdf-7159c49bf7a0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping complete. Dataset saved as 'books_dataset.csv'\n"
          ]
        }
      ]
    }
  ]
}